{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "from  tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import SimpleRNN, LSTM, GRU, Dense, Embedding\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize file names\n",
    "f1 = 'dataset1.csv' # electrons\n",
    "f2 = 'dataset2.csv' #\n",
    "f3 = 'dataset3.csv' #\n",
    "\n",
    "# initialize dataframes: all empty\n",
    "df1 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "files_list = [f1, f2, f3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# funcation to load in df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df1 has loaded in.\n",
      "df2 has loaded in.\n",
      "df3 has loaded in.\n"
     ]
    }
   ],
   "source": [
    "# initialize dataframes: all empty\n",
    "df1 = pd.DataFrame()\n",
    "df2 = pd.DataFrame()\n",
    "df3 = pd.DataFrame()\n",
    "\n",
    "files_list = [f1, f2, f3]\n",
    "\n",
    "def load_df(fname, df, name):\n",
    "  file_path = 'dataset-csv-files/' + fname\n",
    "\n",
    "  # Read the CSV file\n",
    "  new_data = pd.read_csv(file_path)\n",
    "\n",
    "  # If df is still empty, you can directly assign new_data to df\n",
    "  if df.empty:\n",
    "    df = new_data\n",
    "  else:\n",
    "    # Concatenate the new data with the existing DataFrame\n",
    "    df = pd.concat([df, new_data], ignore_index=True)\n",
    "\n",
    "  print(name + ' has loaded in.')\n",
    "  #print(df.shape)\n",
    "  #print(df.head())\n",
    "  #print('\\n')\n",
    "  return df\n",
    "\n",
    "df1 = load_df(f1, df1, 'df1')\n",
    "df2 = load_df(f2, df2, 'df2')\n",
    "df3 = load_df(f3, df3, 'df3')\n",
    "\n",
    "df_list = [df1, df2, df3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# map df values for datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset 1\n",
    "df1_texts = df1['review_text'].values\n",
    "df1_labels = df1['class_index'].map({1: 0, 2: 1}).values\n",
    "\n",
    "# Dataset 2\n",
    "df2_texts = df2['reviews.text'].values\n",
    "df2_labels = df2['reviews.doRecommend'].map({True: 1, False: 0}).values\n",
    "\n",
    "# Dataset 3\n",
    "df3_texts = df3['verified_reviews'].astype(str).values # Your text data\n",
    "df3_labels = df3['feedback'].map({1: 0, 2: 1}).values.astype(int)  # Converting 1,2 to 0,1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# first version of vanilla rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def VanillaRNN(df_texts, df_labels, name):\n",
    "    vocab_size = 5000\n",
    "    embd_len = 32\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(df_texts)\n",
    "    sequences = tokenizer.texts_to_sequences(df_texts)\n",
    "\n",
    "    max_words = max(len(sequence) for sequence in sequences)\n",
    "    x_data = pad_sequences(sequences, maxlen=max_words)\n",
    "\n",
    "    x_temp, x_test, y_temp, y_test = train_test_split(x_data, df_labels, test_size=0.2, random_state=42)\n",
    "    x_train, x_valid, y_train, y_valid = train_test_split(x_temp, y_temp, test_size=0.25, random_state=42)\n",
    "\n",
    "    RNN_model = Sequential(name=\"Simple_RNN\")\n",
    "    RNN_model.add(Embedding(input_dim=vocab_size, output_dim=embd_len, input_length=max_words))\n",
    "    RNN_model.add(SimpleRNN(128))\n",
    "    RNN_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "    RNN_model.compile(loss=\"binary_crossentropy\", optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    history = RNN_model.fit(x_train, y_train, batch_size=64, epochs=11, verbose=1, validation_data=(x_valid.astype(int), y_valid.astype(int)))\n",
    "\n",
    "    score = RNN_model.evaluate(x_test.astype(int), y_test.astype(int), verbose=0)\n",
    "    print(f\"\\n{name} Score---> {score}\")\n",
    "    print()\n",
    "\n",
    "    print(f'Visualize data for: {name}')\n",
    "\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'Model accuracy for {name}')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'Model loss for {name}')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VanillaRNN(df1_texts, df1_labels, 'Dataset1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VanillaRNN(df2_texts, df2_labels, 'Dataset2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VanillaRNN(df3_texts, df3_labels, 'Dataset3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
